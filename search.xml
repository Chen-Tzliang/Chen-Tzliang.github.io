<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[大数据处理平台Hadoop]]></title>
    <url>%2F2019%2F07%2F17%2Fhadoop-1%2F</url>
    <content type="text"><![CDATA[Hadoop发展Hadoop是Doug Cutting（Apache Lucene创始人）开发的使用广泛的文本搜索库。Hadoop起源于Apache Nutch，后者是一个开源的网络搜索引擎，本身也是由Lucene项目的一部分。 Hadoop这个名字不是一个缩写，它是一个虚构的名字。该项目的创建者，Doug Cutting如此解释Hadoop的得名：”这个名字是我孩子给一头吃饱了的棕黄色大象命名的。我的命名标准就是简短，容易发音和拼写，没有太多的意义，并且不会被用于别处。小孩子是这方面的高手。Googol就是由小孩命名的。” Nutch项目开始于2002年，一个可工作的抓取工具和搜索系统很快浮出水面。但他们意识到，他们的架构将无法扩展到拥有数十亿网页的网络。在 2003年发表的一篇描述Google分布式文件系统(简称GFS)的论文为他们提供了及时的帮助，文中称Google正在使用此文件系统。GFS或类似的东西，可以解决他们在网络抓取和索引过程中产生的大量的文件的存储需求。具体而言，GFS会省掉管理所花的时间，如管理存储节点。在 2004年，他们开始写一个开放源码的应用，即Nutch的分布式文件系统(NDFS)。2004年，Google发表了论文，向全世界介绍了MapReduce。2005年初，Nutch的开发者在Nutch上有了一个可工作的MapReduce应用，到当年年中，所有主要的Nutch算法被移植到使用MapReduce和NDFS来运行。 Nutch中的NDFS和MapReduce实现的应用远不只是搜索领域，在2006年2月，他们从Nutch转移出来成为一个独立的Lucene 子项目，称为Hadoop。大约在同一时间，Doug Cutting加入雅虎，Yahoo提供一个专门的团队和资源将Hadoop发展成一个可在网络上运行的系统。在2008年2月，雅虎宣布其搜索引擎产品部署在一个拥有1万个内核的Hadoop集群上。 2008年1月，Hadoop已成为Apache顶级项目，证明它是成功的，是一个多样化、活跃的社区。通过这次机会，Hadoop成功地被雅虎之外的很多公司应用，如Last.fm、Facebook和《纽约时报》。 2008年4月，Hadoop打破世界纪录，成为最快排序1TB数据的系统。运行在一个910节点的群集，Hadoop在209秒内排序了1TB的数据，击败了前一年的297秒冠军。同年11月，谷歌在报告中声称，它的MapReduce实现执行1TB数据的排序只用了68 秒。 在2009年5月，有报道宣称Yahoo的团队使用Hadoop对1 TB的数据进行排序只花了62秒时间。 图1 hadoop发展图 十年前还没有Hadoop，几年前国内IT圈里还不知道什么是Hadoop，而现在几乎所有大型企业的IT系统中有已经有了Hadoop的集群在运行了各式各样的任务。 2006年项目成立的一开始，“Hadoop”这个单词只代表了两个组件——HDFS和MapReduce。到现在的10个年头，这个单词代表的是“核心”（即Core Hadoop项目）以及与之相关的一个不断成长的生态系统。这个和Linux非常类似，都是由一个核心和一个生态系统组成。现在Hadoop俨然已经成为企业数据平台的“新常态”。我们很荣幸能够见证Hadoop十年从无到有，再到称王。 1.2Hadoop的体系结构Hadoop是一个能够对大量数据进行分布式处理的软件框架。具有可靠、高效、可伸缩的特点。Hadoop的核心是HDFS（分布式文件系统）和MapReduce（分布式运算编程框架），Hadoop2.0还包括YARN（运算资源调度系统）。 整个Hadoop的体系结构主要是通过HDFS来实现对分布式存储的底层支持，并通过MR来实现对分布式并行任务处理的程序支持。 本节在具体介绍Hadoop体系结构之前，先用一个流程图介绍Hadoop业务的开发流程。 图2 hadoop业务开发流程图 图3 hadoop的核心组件图]]></content>
      <categories>
        <category>bigdata</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[对自编码及时间序列特征提取的总结]]></title>
    <url>%2F2019%2F07%2F15%2F%E5%AF%B9%E8%87%AA%E7%BC%96%E7%A0%81%E5%8F%8A%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%E7%9A%84%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[介绍时间序列是一种应用广、复杂、维度高的数据形式，是根据客观对象的某些物理量在时间维度上的采样值排列组成的数据序列，按一定的时间走势的数据类型，它客观地记录了所观测对象在各个单位时间点上的状态值。时间序列应用十分的广泛，金融数据，流媒体数据、气象数据、人口普查数据、系统日志数据都是时间序列数据类型。而找寻相似的时间序列频繁模式在时间序列的挖掘中有相当重要的意义，例如，我们要找寻某一个股票的走势，或者对股票进行分析，我们需要找出这只股票的一些频繁的周期，也叫motif查询，就是找出股票中走势相似的某些时间序列片段，那么，我们要找寻这些片段就需要用到距离测量的算法，例如欧式距离，相关系数距离等，然后通过距离阀值找到小于某个阀值的片段。在找寻这些片段时可能要花费大量的时间，这时我们就需要到一些降维的技术来缩短，本文所讨论的是单变量的时间序列，在传统的方法中，我们有paa分段技术，符号化sax，主成分分析法pca等等，这些都是传统的降维方法，本文通过大量文章的阅读，提出了用自编码器研究时间序列特征提取，并大量实验测量降维后的特征向量是否保持原有的时间序列的特征。 定义问题定义motif查找或者进行时间序列片段的相似度计算的时候，降低时间序列的维度对于分析时间序列有着极其重要的作用，时间序列的特征提取能很大限度的减少时间序列距离测量的时间，因此，我们指在研究自编码器对时间序列的特征提取。栈式自编码器特征提取是指利用栈式自编码器对时间序列特征进行提取，对于高维的时间序列的片段，我们通过训练栈式自编码器，使该神经网络能够从输入的时间序列片段中得到一个低维的特征向量的输出，再次，我们利用该低维的特征向量完成motif的查找，并通过特征提取减少通过高维暴力计算找寻motif的时间。最后，通过进行大量实验验证特征提取后的阀值与原始阀值的近似关系。 基本定义时间序列定义：时间序列T是按时间顺序排列的数据点序列。$T=$[($t$$1$,$d$$1$),($t$$2$,$d$$2$),…,($t$$n$,$d$$n$)],其中d是时间$t$的数据项，$n$是时间序列的长度。 时间子序列定义：子序列$T[j:j+m]$ = [$d$$j$,$d$$j+1$,…,$d$$j+m$]是从位置$j$开始，长度为$m$的$T$中的一组连续点，本文只考虑时间序列的数据值。 时间序列motif定义：Motif被描述为重复的模式、频繁的趋势、近似重复的序列、形状、事件或频繁的子序列，所有这些都有相同的目标。Motif旨在发现子序列匹配数量最多的片段，通过提前定义的阀值$\varepsilon$，当所找的子序列片段之间的距离小于$\varepsilon$，就认为这些子序列片段为一个motif，如图1.1。 图1.1 时间序列的两个频繁模式，红色和绿色为一个Motif 栈式自编码器(SAE)定义：是一种无监督性的机器学习算法，其模型结构简单，只具有一个隐含层，网络层神经元之间是全连接的结构，如图1.2所示。研究者们对经典的自动编码器进行了很多优化的研究，提出了多种自动编码器的改良版本，如对原始自动编码器中隐含层的神经单元加以稀疏性的限制，这种网络被称为稀疏自动编码器。将经典的自动编码器进行叠加组合，则构成了栈式自动编码器。 图1.1 自动编码器网络示意图 栈式自动编码器是自动编码器的改进模型，由自动编码器的有效堆叠组合构成。网络模型对数据特征进行逐层的抽象提取，前一层自动编码器的输出(隐含层)结果将用于后一层自动编码器网络的输入，然后将自动编码器的编码部分进行叠加，然后叠加对应的编码部分。栈式自动编码器逐层对网络模型进行训练，每次对一个网络层以贪婪的思想进行训练，训练的算法为无监督的稀疏自动编码器。训练的过程参见图1.2，首先用自动编码器训练原始数据$X$$i$，得到网络模型的参数$W$$1$,$b$$1$和原始数据的一阶特征表示$h$$1$，然后将一阶特征$h$$1$作为下一个自动编码器网络的输入，训练得到第二个自动编码器的参数$W$$2$,$b$$2$和二阶特征表示$h$$2$,训练过程中，保持其他层的参数不变。最后，当训练完成后，通过微调整个网络模型，以改善整个网络模型的学习效果。 栈式自编码器提取时间序列特征的方法1.1 基本思路栈式自编码器广泛应用于无监督特征学习，本文将用于时间序列的特征提取。通过对上述栈式自动编码器的定义，我们可以将高维的时间序列转化为低维特征表示。我们先将时间序列切片然后转化为空间的一个向量，并用自动编码器提取低维特征。 本文利用栈式自编码器的基本思想为：对于一个大规模高维的时间序列，我们将时间序列按子序列切分为大规模的小片段时间序列向量，对大规模时间序列片段向量组成的矩阵进行深层的学习，使用神经网络学习时间点$t$$i$，$t$$m$（$i$和$m$是时间序列的任意时刻）之间的关联，比如$t$$5$，$t$$10$在时间序列中的一种特殊的关联，完成这两个点之间的特征学习，进一步特征提取，去理解多个点之间的关联性,最终学习出这个时间序列的关联特征。经过网络的训练之后，我们能得到一个模型，这个模型的输入是一个高维的时间序列片段，输出是一个低维的时间序列特征向量。模型如下，输入是$T[j:j+m]$,输出是$S[i:i+l]$,其中$m&gt;l$。 $S[i:i+l]=f(T[j:j+m])$ (1) 时间序列特征提取模型 自动编码器将时间序列的高维向量转化为低维向量，并且学习到的低维的特征向量包含了高维时间序列的本质特征，是原始时间序列的良好表示，也是原始时间序列的另一种特征表示，由此得到的特征向量可用于相似度计算以及motif的查找。算法流程分为3个步骤：首先，对时间序列进行预处理，对时间序列按一定的子序列长度进行切分，构建时间序列片段向量的空间模型，每个时间序列片段转化成空间中的一个向量。然后将这些高维的向量组成的矩阵输入到构建好的栈式自编码器中学习，经过逐层的抽象，提取不同抽象程度的低维特征向量。最后，利用传统的motif查找方法对抽取到的低维时间序列特征向量进行motif查找，验证原始方法找寻的motif与栈式特征提取后找寻motif的方法的准确度。 1.2 时间序列的预处理1.2.1 时间序列规范化数据的规范化包括归一化标准化正则化，是一个统称（也有人把标准化作为统称）。 数据规范化是数据挖掘中的数据变换的一种方式，数据变换将数据变换或统一成适合于数据挖掘的形式，将被挖掘对象的属性数据按比例缩放，使其落入一个小的特定区间内，如[-1, 1]或[0, 1]。对属性值进行规范化常用于涉及神经网络和距离度量的分类算法和聚类算法当中。比如使用神经网络后向传播算法进行分类挖掘时，对训练元组中度量每个属性的输入值进行规范化有利于加快学习阶段的速度。对于基于距离度量相异度的方法，数据归一化能够让所有的属性具有相同的权值。 数据规范化的常用方法有三种：最小最大值规范化，z-score标准化和按小数定标规范化。 在本文中由于使用的是sigmoid函数输出的是$[0，1]$，且在对神经网络进行训练的时候，应当把输入的数据处理成适合于网络训练的数据，这里需要对原始的时间序列进行放缩，进行归一化的原因是把各个特征的尺度控制在相同的范围内，这样可以便于找到最优解。文章用的是最小最大值规范化，由于使用的是sigmoid函数，因此对时间序列进行最小最大值规范化，公式如下： $x^*=\frac{x-max}{max-min}$ (2) 公式（2）是最小最大值规范化的公式，该规范化将时间序列数据进行缩放，使数据范围在区间[0，1]上,对时间序列的伸缩变换的目的是使其不同度量之间的特征具有可比性，同时不改变原始数据的分布。 1.2.2 适合神经网络的时间序列子序列的片段向量矩阵公式（2）已经对时间序列数据进行了最小最大值规范化，但是这样的时间序列数据并不能适合于我们的自编码器神经网络的训练，因此我们还需要对数据进行处理，使构建的时间序列数据集适合于自编码器的训练。对时间序列的特征提取，或者说是motif查找时都是研究的时间序列的子序列，因此，我们在用栈式自编码器进行特征提取时，操作的应该是子序列，为了减少过拟合的问题，我们在截取时间序列片段时用了滑动窗口，大小是64，子序列片段设置为256，因此我们对一个完整的大的时间序列进行片段的切分，每256维构建一个向量空间模型，滑动窗口是64，就是每滑动64个维度进行一个向量模型的截取，最后这些截取的向量空间模型构成一个向量矩阵作为自编码器的训练数据集。 1.3 栈式自编码器栈式自编码器是一种多层的前传神经网络，可以用来对数据进行特征提取，同样也可以用于时间序列的特征提取，自编码器已经广泛运用于深度学习中，在进行时间序列特征提取时，本文使用了1个输入层、2个隐含层以及一个输出层组成的神经网络。以第一个隐含层为例，输入是一个256维的时间序列子片段向量$x$，输出层设置为128维的向量$z$，该一层$z$是自编码器降维后的特征向量，然后输出层是根据特征向量自编码器重构的256维向量$\hat{x}$，该隐藏层和输出层的激活情况公式为 $z=sigmoid(Wx+b)$ (3) $\hat{x}=sigmoid(W^Ty+b^‘)$ (4) 其中，$sigmoid(x)=1/(1-e^{-x})$为Sigmoid函数，其中$W$是自编码器的权重矩阵，$W^T$是解码器的权重矩阵，$b$和$b^‘$是偏置量。 作为无监督学习，自动编码器与监督学习不同，自动编码器的目标是输出层尽量重构输入层的状态，理想的情况下，隐藏层的特征能正确重构一个完全相同的输入层的时间序列片段，所以说隐藏层所得的特征向量即为原始时间序列的良好表示，但是重构不能保证100%的正确率，因此隐藏层作为提取的特征向量还是存在有一点误差，该误差来源于重构误差，训练时要使编码器尽可能减少重构误差。所以，自编码器重构的目标函数可以得出，公式为 $\left{ W,b,W^T,b^‘ \right}=argmin\,Loss(x,\hat{x})=||x-\hat{x}||^2$ (5) 其中，$Loss(x,\hat{x})$是损失函数，也叫重构误差，它的作用是衡量最后输出层与输入层重构的情况，我们自编码器要找寻的就是该误差的最小值。 隐藏层$z$的维度是小于输入层$x$的维度的，所以隐藏层$z$可以学习到输入样本的低维表示，并且通过解码能够包含与高维表示相同的信息。使用没有标注的数据集$X$，进行自动编码的无监督学习，最后对于任何输入向量$x$，通过训练好的模型进行计算，得到隐藏层向量$z$,即输入向量的一个低维编码。 自动编码器的权重训练采用随机梯度下降算法，使用公式为 $W\leftarrow W-\eta\frac{\partial Loss(X,\hat{X})}{\partial W}$ (6) 该公式是梯度下降公式，用来更新权重矩阵，其中:$\eta$为更新的步长；其他参数$b,W^T,b^‘$采用相同的方式更新。本文更新的步长设置为0.001。 1.4 motif的查找方法通过大量文章的阅读，发现前人在motif的找寻中提供了大量的方法，为了方便进行结果的分析和比较，本文选用了一种思路就是matrix profile的方式，该方法是想计算出所有的距离然后形成的矩阵，例如，有一个时间序列T,T0是长度为m，起始位置为0的T的子序列，当用该T0与时间序列T中的所有子序列段作比较时，能得出一个距离向量d0;同理可得子序列段T1,T2,…,Tn-m+1的距离向量d1,d2,…,dn-m+1;这些距离向量共同构成了一个矩阵D。该矩阵保留了所有子序列之间的关系，我们要查询子序列之间的关系时，只需要通过矩阵的下标值就能得到相应子序列段的距离，例如:矩阵D[3][10]的元素值，即为T3与T10两个子序列片段的欧几里得距离，欧式公式为 d(x,y)= \sqrt{(x_1-y_1)^2+(x_2-y_2)^2+...+(x_n-y_n)^2}= \sqrt{\sum_{i=1}^n(x_i-y_i)^2} (7) 欧式公式是度量两个子序列之间的距离的，当距离越小时，代表两个片段越相似，同理，当距离越大时，两个片段越不相似。 栈式自编码器提取时间序列特征实现2.1 数据预处理的实现在这一步中我们着重构建一个适合于神经网络学习的数据集，构建一个适合于神经网络输入的向量矩阵，我们的目标是特征提取，当然我们不能直接输入整个时间序列，全部一次性进行特征提取，因此我们从子序列入手，在这一步，我们先读取准备好的时间序列文本。该数据集含有40m，初步估计有几百万个时间点数据。预处理的第一步是进行最小值最大值规范化，之后按长度m为256，滑动窗口window为64进行切分该时间序列T，切分后的时间序列应该是一个个长为256的子序列片段，T0,T64,…，然后按每k个样本进行合成矩阵，之后把这些矩阵合并成一个3维的空间模型F，这就完成了数据集的整理，流程如算法1 算法1 Procedure dataProcess(filePath) Input:时间序列T Output:适合于神经网络的3维空间模型F，包含大小为k的向量组成矩阵 T $\leftarrow$ getFile(filePath) t $\leftarrow$ MaxMinScore(T) M $\leftarrow$ TsSplitAndCombineMatrix(m,window,T) F $\leftarrow$ MCombineSpace(k) return F 对于算法1，我们最终得到的是一个3维的空间模型，getFile()函数可以读取时间序列文件返回原始时间序列，MaxMinScore()对时间序列进行最小值最大值规范化处理。TsSplitAndCombineMatrix()中的m是指子序列的片段大小，window是指滑动窗口大小，该方法返回的是以子序列向量组成的向量矩阵。MCombineSpace()中的k是每个合成矩阵的大小，返回的F是合成矩阵的集合。该空间模型也是神经网络的输入数据集。 2.2 栈式自编码器实现栈式自编码器的实现是本文的核心的步骤，首先我们的目标是输入一个256维的时间序列子片段，通过该栈式自编码器能够提取相应的特征向量。我们得设置一些神经网络的参数，在实现过程中，我们设置一个输入层，两个隐藏层，一个输出层，该栈式自编码网络提取的特征向量是第二个隐藏层，我们的输入层设置为256，第一个隐藏层的输出设置为128，然后激活函数用的是sigmoid()函数。 因此，第一层输入为X，第一层自编码器权重矩阵W1_e应该是[256,128]，相应的第一层的解码器权重矩阵W1_d应该是[128,256],第一层的偏置量b1_e应该是[128],第一层的解码器偏置量b1_d应该是[256]，第一层的输出为Z1，根据公式(3)(4)原理实现第一层，根据公式(5)来编写这一层的重构误差，利用BP梯度下降公式(6)进行这一层的网络学习训练，具体的算法2如下 算法2 Procedure firstHidden(F) Input:W1_e，W1_d，b1_e，b1_d Output:训练得到的第一层编码器的权重矩阵和偏置量（W1_e，W1_d，b1_e，b1_d） (W1_e，W1_d，b1_e，b1_d)$\leftarrow$ WeightInitialization(W1_e，W1_d，b1_e，b1_d) for each $X_i$ in F:{ Z1 $\leftarrow$ Encoder1(W1_e,b1_e,$X_i$) $\hat{X_i}$ $\leftarrow$ Decoder1(W1_d,b1_d,Z1) loss=caluteLoss($X_i$,$\hat{X_i}$) updateWeightBylearn() } return W1_e，W1_d，b1_e，b1_d 上述的算法2中WeightInitialization()函数是对W1_e，W1_d，b1_e，b1_d这些值初始化并根据均值为0,方差为1给这些权重初始值，Encoder()函数是编码器实现了公式(3)，返回的是输入的特征提取后的特征向量。Decoder()函数是解码器实现了公式(4)，返回的是特征向量重构的输入时间序列片段，caluteLoss()函数实现了公式(5)重构误差的计算，updateWeightBylearn()通过BP梯度下降公式(6)更新权重。算法2返回了一些训练好的权重参数可用于第二层的训练。 第二层隐藏层的输入来源于第一层隐藏层的输出，第二层的输入为Z1，由第一层的自编码器得到。同理，第二层的自编码器权重矩阵W2_e应该是[128,64]，相应的第二层的解码器是[64，128],第一层的偏置量b2_e应该是[64],第一层的解码器偏置量b2_d应该是[128]，第一层的输出为Z2，根据公式(3)(4)原理实现第二层，根据公式(5)来编写这一层的重构误差，利用BP梯度下降公式(6)进行这一层的网络学习训练，具体的算法3如下 算法3 Procedure secondHidden(W1_e,b1_e,F) Input:W1_e，b1_e,F,W2_e，W2_d，b2_e，b2_d Output:训练得到model (W2_e，W2_d，b2_e，b2_d)$\leftarrow$ WeightInitialization(W2_e，W2_d，b2_e，b2_d) for each $X_i$ in F:{ $Z1$ $\leftarrow$ Encoder1(W1_e,b1_e,$X_i$) $Z2$ $\leftarrow$ Encoder2(W2_e,b2_e,$Z1$) $\hat{Z1}$ $\leftarrow$ Decoder2(W1_d,b1_d,$Z2$) loss=caluteLoss($Z1$,$\hat{Z1}$) updateWeightBylearn() } model $\leftarrow$ saveModel() return model 上述算法3WeightInitialization()函数也是初始化第二层的权重，Z1是第一层隐藏层的编码，第二层是对第一层的重复利用，只不过输入的是第一层的编码，重构的是第一层的编码，同样该层也是运用BP梯度下降来更新权重，在训练完成后，我们需要通过saveModel()函数将模型保存下来。 通过算法1、2、3，我们已经完成了神经网络的训练，最终我们得到一个模型，该模型的功能如公式(1),其中m是256，l是64，所以当我们输入一个时间序列的子序列长度为256时，我们能通过模型直接得到64维的自编码器提取后的特征向量，至此神经网络的训练基本上完成，以下是验证该网络的效果和准确率。 2.3 原始方法找寻motif原始方法暴力求解距离矩阵可以找到正确的motif，这里我们的原始方法同样用的是欧式距离，当然我们也可以用相关系数的方法来找出motif，但是这里我们采用的是欧式距离，因为我们仅仅用来检测神经网络的降维效果。为了简单起见，我们用的测试数据点大约是60000个点，为了避免重复计算，我们建立滑动窗口，大小同样为64，所谓的暴力算法，就是使用循环来找寻这个距离矩阵，具体的原理可以参考上述1.4motif的查找方法，最后我们给原始方法设置一个阀值$\varepsilon$，例如，当我们找到矩阵中小于$\varepsilon$的值是D[3][10],矩阵的下标是记录的起始位置，当然这个位置得乘上滑动窗口，D[3][10]代表的是T[3 64:3 64+256]跟T[10 64:10 64+256]的两个子序列的距离，因为它们之间的距离小于$\varepsilon$，因此这两个子序列片段为一个motif。原始方法找寻motif的方法如算法4 算法4 Procedure findMotif(filePath) Input:时间序列T Output:motif T $\leftarrow$ getFile(filePath) M $\leftarrow$ TsSplitAndCombineMatrix(m,window,T) for i in range(0,len(M)){ for j in range(i,len(M)){ dist $\leftarrow$ calculateEd(M[i],M[j]) D[i][j] $\leftarrow$ dist } } motif $\leftarrow$ lessThanVarepsilon(D) return motif 上述算法是原始找寻motif的方法，T是读取文件得到的时间序列，M是以大小为256对时间序列进行切分，滑动窗口为64的向量矩阵，矩阵的第一行就是以0开始的大小为256的子序列，第二行是64开始的，长度为256的第二个子序列，依次下去。calculateEd()函数计算的是两个子序列的欧几里得距离，D是所有子序列之间的距离矩阵，lessThanVarepsilon()是找出距离矩阵D中小于阀值$\varepsilon$的值，即为motif。 2.4 特征提取后找寻的motif特征提取找寻motif只需在中间添加一个步骤就是神经网络提取特征的那一个步骤,把上面的算法加以改进就是神经网络降维找寻motif的方法了，如算法5 算法5 Procedure findMotif(filePath) Input:时间序列T Output:motif T $\leftarrow$ getFile(filePath) M $\leftarrow$ TsSplitAndCombineMatrix(m,window,T) for i in range(0,len(M)){ for j in range(i,len(M)){ S1 $\leftarrow$ model(M[i]) S2 $\leftarrow$ model(M[j]) dist $\leftarrow$ calculateEd(S1,S2) D[i][j] $\leftarrow$ dist } } motif $\leftarrow$ lessThanVarepsilon(D) return motif 上述算法添加了特征提取的model()那个步骤，目的是验证看神经网络进行特征提取之后特征向量跟原始时间序列是否保持原有的特征，也想通过代码验证这种自编码的方法在实现特征提取是否为原始序列的另一种良好表述。 实验结果及分析]]></content>
      <categories>
        <category>论文（学术）</category>
      </categories>
      <tags>
        <tag>论文（学术）</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2019%2F07%2F10%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
