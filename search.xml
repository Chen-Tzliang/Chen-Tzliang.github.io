<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[大数据处理平台Hadoop（三）]]></title>
    <url>%2F2019%2F08%2F05%2Fhadoop-3%2F</url>
    <content type="text"><![CDATA[大数据处理平台Hadoop（三）Hadoop的远程登录工具及jdk的安装通过上一章的学习，相信读者朋友能够搭建出centos7的hadoop节点，要深入学习hadoop，请读者务必认真搭建好Hadoop的节点，毕竟现实中的我们没有真实的hadoop集群，我们只能靠电脑虚拟出来，通过虚拟机技术来模拟服务器。请读者朋友记住，这次我们需要的是4台虚拟机，请认真的配置好再进行下面章节的学习。下面的这一章，我们介绍的是用远程登录工具登录我们的节点，并且在节点安装jdk。 Hadoop的远程登录工具对于远程登录工具，我们的选择是多种多样的，为什么要用远程登录工具呢，我们都知道，在工作或者其他环境下，我们的服务器是放置在机房里的，那么我们对这些服务器进行操作要怎么办，这个时候我们必须得通过远程登录工具连接上我们的服务器，也称hadoop集群，我们通过远程登录工具来对这些集群进行操作。远程登录工具有很多，例如putty，SecureCRT等。下面我们用的是SecureCRT，对于macos用户可以直接用终端连接，但是本文用的是SecureCRT。 SecureCRT登录服务器1.创建新的连接 或许不是所有的远程登录工具都一样，但是大致的操作都是一样的，当我们要创建新的连接的时候，需要点击以下的quick connection来创建新的连接。 2.输入节点的ip和用户名 当用户创建了新的连接时是需要用户输入相应的ip和用户名的，这里的ip估计阅读过第二章的朋友们应该不会陌生，上一章我们为什么要静态ip，目的之一就是为了用远程工具登录特定的节点。这里的ip就是虚拟节点的ip地址，用户名就是root。 3.输入密码 完成新连接的创建之后，我们要打开这个连接，必须要输入密码，这个密码就是用户名的root密码，当我们输入密码就可以登录节点了 4.成功登录服务器 下图就是当你成功登录时会出现的页面，当用户能看见出现你的虚拟服务器的终端界面时就表明你成功登录了服务器，并且能够进行服务器的操作。 上面是我们使用工具远程登录服务器的操作过程，很多时候我们操作集群的时候都是通过远程登录工具来完成的，以后我们工作中也会经常利用到远程登录工具的。 #### jdk的安装 下面我们在搭建hadoop集群运用hdfs和mapreduced的时候，必须要在每个节点上安装好jdk，下面我们将进行单个节点jdk的安装，先从一个节点来进行配置。 #### 虚拟节点安装jdk 1.上传jdk安装文件 在虚拟节点上安装jdk，我们需要下载jdk-8u65-linux-x64.tar.gz，具体的网址为 https://www.oracle.com/technetwork/java/javase/downloads/java-archive-javase8-2177648.html 下载好jdk-8u65-linux-x64.tar.gz后我们可以通过SecureCRT把文件上传至虚拟节点。 使用SecureCRT的sftp可以上传文件，具体其他的上传方法可以自行网上搜索。 2.创建/soft目录用来存放各种组件1$&gt;mkdir /soft 3.tar开jdk文件 把上传的jdk安装文件tar开保存至/soft刚刚通过上传的文件存放在了～目录下，我们登录虚拟节点后通过ls可以看见jdk-8u65-linux-x64.tar.gz,表示jdk安装包上传成功，下面我们需要安装jdk，这时我们需要使用命令行把刚刚上传好的文件tar开，并且要放进/soft目录，具体的命令行如下1$&gt;tar -zxvf jdk-8u65-linux-x64.tar.gz -C /soft tar开jdk安装包后进入/soft文件夹会发现jdk1.8.0_65文件，这个就是我们要tar开好的文件，下面为了方便，我们创建软连接，软连接相当于快捷方式，我们用jdk代表jdk1.8.0_65这个文件，具体命令行如下12$&gt;cd /soft$&gt;ln -s jdk1.8.0_65/ jdk 这样我们创建的jdk软连接就指向了jdk1.8.0_65文件夹。 4.配置jdk环境变量 jdk的环境变量在/etc/profile中配置，我们通过编辑 /etc/profile，在这个文件的末尾添加上123#javaexport JAVA_HOME=/soft/jdkexport PATH=$PATH:$JAVA_HOME/bin 在加入这两行配置之后，我们需要使文件生效，我们要用命令行 1$&gt;source /etc/profile 5.测试jdk安装 安装完成后像在win10或者macos上测试一样，使用java -verion，如果出现以下界面，就表示jdk安装成功！ 这一篇文章同样是进行虚拟节点hadoop环境的配置，在本章中介绍了一些远程登录工具，我们能通过这些工具登录进去我们的虚拟节点或者公司的服务器中，还有就是介绍了节点的jdk安装，同时希望读者认真阅读，并在自己节点上安装，同时应该注意的是，每一个节点都应该安装jdk，所以请读者朋友在你各个节点上都配置好。]]></content>
      <categories>
        <category>bigdata</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据处理平台Hadoop（二）]]></title>
    <url>%2F2019%2F07%2F24%2Fhadoop-2%2F</url>
    <content type="text"><![CDATA[大数据处理平台Hadoop（二）Hadoop节点的准备通过上一章的学习和理解，相信读者朋友们和我一样已经理解了hadoop的发展和hadoop是个什么平台，hadoop的核心是hdfs分布式文件系统和mapreduce。但是，我们要使用分布式文件系统(HDFS)和mapreduce的时候必须进行环境的搭建，在进行hadoop的配置时，由于涉及的是分布式，因此我们需要多台机器的协助，虽然我们不能同时拥有多台电脑，但是，一些虚拟机技术带给我们学习Hadoop的方便。在准备学习hadoop平台时，希望读者朋友先准备4台虚拟机，当然要是读者朋友们有条件的话，可以得到节点也可以忽略虚拟机的安装。 linux系统及虚拟机安装对于linux系统及虚拟机不熟悉的用户，可以先网上自行搜索学习，hadoop的使用是需要用到linux系统的，我这里使用的是centos7操作系统，在学习hadoop时请务必准备4台虚拟机，当然要是电脑配置不够高的朋友们，有条件的话可以准备4个节点。本人用的是Mac OS系统，所以安装过程和win10可能稍有不同，但是在centos7的安装也是一样的操作。win10用的是vmware，下面我仅仅介绍Mac用户的linux安装，mac用户和win10用户只是在虚拟机工具上稍有不同，但是在linux的安装上是一致的下面我们来介绍Linux系统centos7的安装。 centos7安装win10和Mac用户安装centos7的时候大同小异，只不过是软件的不一样，Mac用户用虚拟软件是wmware fushion，下面我们进行安装 1.创建新的虚拟机 2.虚拟机的配置 选择准备好的镜像文件及进行虚拟机的配置，一直进行继续下去，操作系统选择centOS7 64位，网络方式为NAT模式，内存根据自己爱好进行分配，条件允许一般为1g-2g最好，硬盘根据需要自己来选择，还有其他设备的选择一般选择默认方式，最后结果如下图 对于使用win10的同学，可以网上搜索wmware workstation的使用教程进行镜像的加载。 3.下面是进行centos7的安装 这一步骤无论是win10用户还是mac用户都是一样的方法，在加载完镜像文件后，虚拟机会出现以下画面我们选择Install CentOS 7并进入等待的界面，具体的如下所示 选择Install CentOS 7后将进入以下的界面请读者朋友们进行耐心的等待 要是安装成功的情况下，将出现以下的界面，那么我们就可以正式的进行操作系统CentOS 7的安装了,在下面进行centos 7安装的环境时，一些选项是要进行必要的解析的，界面如下所示，当然在以下界面之前还有个语言的选择，我们可以选择简体中文 上面的选项中，一般我们都是选择默认的选项，但是这里我们在软件选择的是最小化界面，最小化界面是没有界面化，一般我们的虚拟机充当节点时一般都不需要界面化，安装位置这个选项中，我们选择的是自动分区，网络和主机名时，我们点进去，可以选择打开网络，主机名我这里更改为Hadoop201，然后就可以进行开始安装了。 在这里我们看见下面的蓝色条是安装的进度，在安装的期间，我们可以设置自己的ROOT密码，选择第一个按钮，进去设置ROOT的密码，这个密码是你待会安装完成进入centos 7的密码，请务必要记住，安装好了之后点击重启，如果重启之后出现以下界面就表示安装已经成功，输入用户名root，密码刚刚设置的密码，就可以登录进centos 7了。 4.Hadoop节点的工具配置 (1)完成了centos 7的安装后，我们还需要进行一系列的配置，这对于建立hadoop分布式集群来说意义相当重要，节点的一些工具我们需要准备一下，在配置后环境后，我们使用ifconfig命令是不可行的，说明节点缺少了net-tools工具，我们需要安装net-tools工具，具体的命令如下1$&gt;yum install -y net-tools.x86_64 (2)安装完成后就可以通过ifconfig查看centos 7的IP地址了，下面还需要安装linux的编写工具，都是个人的喜好，个人喜欢linux的编写工具是nano，安装的代码如下1$&gt;yum install -y nano 对于其他读者感兴趣或者对自己进行节点操作有用的工具，读者可自行上网搜索并下载。 5.静态IP配置 下面将进行节点的静态IP的配置，这一步比较重要，也是hadoop集群布置最基本的一个步骤，相信读者们在ping百度网站时能ping得通，我们在安装虚拟机时选择的是动态IP，但是对于hadoop集群来说，每一个节点都应该有其不可变的地址才能搭建hadoop集群，所以我们需要配置静态的IP地址。 (1)在进行静态IP的配置时，我们主要配置的是centos 7下的ifcfg-ens33网卡，这个网卡的配置，我们可以通过命令行进入，命令行如下1$&gt;nano /etc/sysconfig/network-scripts/ifcfg-ens33 (2)进入ifcfg-ens33的编辑界面后，我们需要进行几个修改，在BOOTPROTO这一项中我们修改为static，ONBOOT这一项我们改为yes，并在最后面加上三行，IPADDR,NETMASK,GATEWAY,下面我将进行这3个参数的讲解，NETMASK我们可以用255.255.255.0，IPADDR是你要设置的静态IP地址，这个地址是根据你虚拟网卡的网关地址的桥段来设置的，怎么查看虚拟网卡的IP呢，这里我们回到MAC系统的终端，输入ifconfig，看见vmnet8，这个就是虚拟网卡，看见虚拟网卡的inet，如下图 这里的vmnet8的IP地址是192.168.38.1，因此我们的IPADDR可以设置为192.168.38.101，GATEWAY的配置比较需要注意的是要跟虚拟网卡在同一桥段，设置为192.168.38.2，同一桥段是指192.168.38.x，之前3个必须是一样的。最后ifcfg-ens33的信息如下 编写完成后保存退出即可。 (3)设置dns，打开/etc/resolv.conf，编辑里面的信息，在里面添加namesever，具体命令行和图如下 命令行1$&gt;nano /etc/resolv.conf 配置如下图片 (4)重启网络，命令行如下 1$&gt;/etc/init.d/network restart (5)ping百度网站，要是能ping通百度的网址，则说明我们的静态IP配置成功。 这一篇文章是进行hadoop集群搭建的基础篇，当我们第一个节点搭建成功时，我们还需要另外的3个节点，这3个节点的搭建跟第一个节点的搭建是一样的，要是读者朋友懂得使用vmware进行节点的克隆，可以克隆3个这样的节点，并更改IP地址，这里的IP最后是连续的，例如第一个节点的IP是192.168.38.101，主机名是hadoop101，第二个节点的IP是192.168.38.102，主机名是hadoop102，依次下去，当然不会虚拟机克隆的读者也可以使用重复操作配置另外的3台机器，这样出错的概率会更加小。]]></content>
      <categories>
        <category>bigdata</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据处理平台Hadoop（一）]]></title>
    <url>%2F2019%2F07%2F17%2Fhadoop-1%2F</url>
    <content type="text"><![CDATA[简要介绍作为本公众号的一个新的分支，计算机视觉这件小事一般记录的是深度学习和人工智能的学习笔记，今天将为大家献上大数据及大数据处理平台，分布式的一些笔记，以上是大数据平台关于hadoop的简介，以后将带领大家一步步走入大数据的学习，在学习前，我们应该先了解hadoop的简介，在接下来的一些章节，将为大家讲解hadoop的搭建安装，还有一些大数据工具，Hbase，Hive，Spark，zookeeper等等。 大数据处理平台Hadoop（一）Hadoop发展Hadoop是Doug Cutting（Apache Lucene创始人）开发的使用广泛的文本搜索库。Hadoop起源于Apache Nutch，后者是一个开源的网络搜索引擎，本身也是由Lucene项目的一部分。 Hadoop这个名字不是一个缩写，它是一个虚构的名字。该项目的创建者，Doug Cutting如此解释Hadoop的得名：”这个名字是我孩子给一头吃饱了的棕黄色大象命名的。我的命名标准就是简短，容易发音和拼写，没有太多的意义，并且不会被用于别处。小孩子是这方面的高手。Googol就是由小孩命名的。” Nutch项目开始于2002年，一个可工作的抓取工具和搜索系统很快浮出水面。但他们意识到，他们的架构将无法扩展到拥有数十亿网页的网络。在 2003年发表的一篇描述Google分布式文件系统(简称GFS)的论文为他们提供了及时的帮助，文中称Google正在使用此文件系统。GFS或类似的东西，可以解决他们在网络抓取和索引过程中产生的大量的文件的存储需求。具体而言，GFS会省掉管理所花的时间，如管理存储节点。在 2004年，他们开始写一个开放源码的应用，即Nutch的分布式文件系统(NDFS)。2004年，Google发表了论文，向全世界介绍了MapReduce。2005年初，Nutch的开发者在Nutch上有了一个可工作的MapReduce应用，到当年年中，所有主要的Nutch算法被移植到使用MapReduce和NDFS来运行。 Nutch中的NDFS和MapReduce实现的应用远不只是搜索领域，在2006年2月，他们从Nutch转移出来成为一个独立的Lucene 子项目，称为Hadoop。大约在同一时间，Doug Cutting加入雅虎，Yahoo提供一个专门的团队和资源将Hadoop发展成一个可在网络上运行的系统。在2008年2月，雅虎宣布其搜索引擎产品部署在一个拥有1万个内核的Hadoop集群上。 2008年1月，Hadoop已成为Apache顶级项目，证明它是成功的，是一个多样化、活跃的社区。通过这次机会，Hadoop成功地被雅虎之外的很多公司应用，如Last.fm、Facebook和《纽约时报》。 2008年4月，Hadoop打破世界纪录，成为最快排序1TB数据的系统。运行在一个910节点的群集，Hadoop在209秒内排序了1TB的数据，击败了前一年的297秒冠军。同年11月，谷歌在报告中声称，它的MapReduce实现执行1TB数据的排序只用了68 秒。 在2009年5月，有报道宣称Yahoo的团队使用Hadoop对1 TB的数据进行排序只花了62秒时间。 图1 hadoop发展图 十年前还没有Hadoop，几年前国内IT圈里还不知道什么是Hadoop，而现在几乎所有大型企业的IT系统中有已经有了Hadoop的集群在运行了各式各样的任务。 2006年项目成立的一开始，“Hadoop”这个单词只代表了两个组件——HDFS和MapReduce。到现在的10个年头，这个单词代表的是“核心”（即Core Hadoop项目）以及与之相关的一个不断成长的生态系统。这个和Linux非常类似，都是由一个核心和一个生态系统组成。现在Hadoop俨然已经成为企业数据平台的“新常态”。我们很荣幸能够见证Hadoop十年从无到有，再到称王。 Hadoop的体系结构Hadoop是一个能够对大量数据进行分布式处理的软件框架。具有可靠、高效、可伸缩的特点。Hadoop的核心是HDFS（分布式文件系统）和MapReduce（分布式运算编程框架），Hadoop2.0还包括YARN（运算资源调度系统）。 整个Hadoop的体系结构主要是通过HDFS来实现对分布式存储的底层支持，并通过MR来实现对分布式并行任务处理的程序支持。 本节在具体介绍Hadoop体系结构之前，先用一个流程图介绍Hadoop业务的开发流程。 图2 hadoop业务开发流程图 图3 hadoop的核心组件图 Hadoop Common Hadoop Common 为Hadoop 整体架构提供基础支撑性功能，主要包括了文件系统（File System）、远程过程调用协议（RPC）和数据串行化库（Serialization Libraries）。 Hadoop Distributed File System(HDFS) HDFS是一个适合构建廉价计算机集群之上的分布式文件系统，具有低成本、高可靠性、高吞吐量的特点，由早期的NDFS演化而来。 MapReduce MapReduce是一个编程模型和软件框架，用于在大规模计算机集群上编写对大数据进行快速处理的并行化程序。 HBase HBase是一个分布式的、面向列的开源数据库，不同于一般的关系数据库，它是一个适合于非结构化大数据存储的数据库。 HCatalog HCatalog是一个用于管理hadoop产生数据的表存储管理系统。它提供了一个共享的数据模版和数据类型的机制，并对数据表进行抽象以方便用户仅需要关注数据结构设计而不需要考虑数据是如何存储的，同时支持hadoop不同数据处理工具之间的互联互通。 Hive Hive是一个基于hadoop的数据仓库工具，它可以将结构化的数据文件映射为一张数据库表，并提供强大的类SQL查询功能，可以将SQL语句转换为MapReduce任务进行运行。 Pig Pig是一个用于大数据分析的工具，包括了一个数据分析语言和其运行环境。Pig的特点是其结构设计支持真正的并行化处理，因此适合应用于大数据处理环境。 Sqoop Sqoop是一款用于hadoop系统与传统数据库间进行数据交换的工具，可以用于传统数据库（如MYSQL、Oracle）中数据导入HDFS或MapReduce，并将处理后的结果导出到传统数据库中。 Avro Avro是一个基于二进制数据传输高性能中间件，可以做到将数据进行序列化，适用于远程或本地大批量数据交互。 Chukwa Chukwa是一个分布式数据收集和分析系统，用于监控大型分布式系统。Chukwa基于HDFS和MapReduce构建而成，并提供了一系列工具用于显示、监控、分析系统运行数据。 Ambari Ambari是一个用于安装、管理、监控hadoop集群的web界面工具。目前已支持包括MapReduce、HDFS、HBase在内的几乎所有hadoop组件的管理。 ZooKeeper ZooKeeper是一个分布式应用程序协调服务器，用于维护hadoop集群的配置信息、命名信息等，并提供分布式锁同步功能和群组管理功能。 Hadoop的特点（1）支持超大文件 一般来说，HDFS存储的文件可以支持TB和PB级别的数据。 （2）检测和快速应对硬件故障 在集群环境中，硬件故障是常见性问题。因为有上千台服务器连在一起，故障率高，因此故障检测和自动恢复是HDFS文件系统的一个设计目标。假设某一个DataNode节点挂掉之后，因为数据备份，还可以从其他节点里找到。NameNode通过心跳机制来检测DataNode是否还存在。 （3）流式数据访问 HDFS的数据处理规模比较大，应用一次需要大量的数据，同时这些应用一般都是批量处理，而不是用户交互式处理，应用程序能以流的形式访问数据库。主要的是数据的吞吐量，而不是访问速度。访问速度最终是要受制于网络和磁盘的速度，机器节点再多，也不能突破物理的局限，HDFS不适合于低延迟的数据访问，HDFS的是高吞吐量。 （4）简化的一致性模型 对于外部使用用户，不需要了解Hadoop底层细节，比如文件的切块，文件的存储，节点的管理等。一个文件存储在HDFS上后，适合一次写入、多次写出的场景once-write-read-many。因为存储在HDFS上的文件都是超大文件，当上传完这个文件到Hadoop集群后，会进行文件切块、分发、复制等操作。如果文件被修改，会导致重新执行这个过程，而这个过程耗时是最长的。所以在Hadoop里，不允许对上传到HDFS上文件做修改（随机写），在2.0版本可以在后面追加数据。但不建议。 （5）高容错性 数据自动保存多个副本，副本丢失后自动恢复。可构建在廉价机上，实现线性（横向）扩展，当集群增加新节点之后，NameNode也可以感知，将数据分发和备份到相应的节点上。 （6）商用硬件 Hadoop并不需要运行在昂贵且高可靠的硬件上，它是设计运行在商用硬件的集群上的，因此至少对于庞大的集群来说，节点故障的几率还是非常高的。HDFS遇到上述故障时，被设计成能够继续运行且不让用户察觉到明显的中断。]]></content>
      <categories>
        <category>bigdata</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[对自编码及时间序列特征提取的总结]]></title>
    <url>%2F2019%2F07%2F15%2F%E5%AF%B9%E8%87%AA%E7%BC%96%E7%A0%81%E5%8F%8A%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%E7%9A%84%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[介绍时间序列是一种应用广、复杂、维度高的数据形式，是根据客观对象的某些物理量在时间维度上的采样值排列组成的数据序列，按一定的时间走势的数据类型，它客观地记录了所观测对象在各个单位时间点上的状态值。时间序列应用十分的广泛，金融数据，流媒体数据、气象数据、人口普查数据、系统日志数据都是时间序列数据类型。而找寻相似的时间序列频繁模式在时间序列的挖掘中有相当重要的意义，例如，我们要找寻某一个股票的走势，或者对股票进行分析，我们需要找出这只股票的一些频繁的周期，也叫motif查询，就是找出股票中走势相似的某些时间序列片段，那么，我们要找寻这些片段就需要用到距离测量的算法，例如欧式距离，相关系数距离等，然后通过距离阀值找到小于某个阀值的片段。在找寻这些片段时可能要花费大量的时间，这时我们就需要到一些降维的技术来缩短，本文所讨论的是单变量的时间序列，在传统的方法中，我们有paa分段技术，符号化sax，主成分分析法pca等等，这些都是传统的降维方法，本文通过大量文章的阅读，提出了用自编码器研究时间序列特征提取，并大量实验测量降维后的特征向量是否保持原有的时间序列的特征。 定义问题定义motif查找或者进行时间序列片段的相似度计算的时候，降低时间序列的维度对于分析时间序列有着极其重要的作用，时间序列的特征提取能很大限度的减少时间序列距离测量的时间，因此，我们指在研究自编码器对时间序列的特征提取。栈式自编码器特征提取是指利用栈式自编码器对时间序列特征进行提取，对于高维的时间序列的片段，我们通过训练栈式自编码器，使该神经网络能够从输入的时间序列片段中得到一个低维的特征向量的输出，再次，我们利用该低维的特征向量完成motif的查找，并通过特征提取减少通过高维暴力计算找寻motif的时间。最后，通过进行大量实验验证特征提取后的阀值与原始阀值的近似关系。 基本定义时间序列定义：时间序列T是按时间顺序排列的数据点序列。$T=$[($t$$1$,$d$$1$),($t$$2$,$d$$2$),…,($t$$n$,$d$$n$)],其中d是时间$t$的数据项，$n$是时间序列的长度。 时间子序列定义：子序列$T[j:j+m]$ = [$d$$j$,$d$$j+1$,…,$d$$j+m$]是从位置$j$开始，长度为$m$的$T$中的一组连续点，本文只考虑时间序列的数据值。 时间序列motif定义：Motif被描述为重复的模式、频繁的趋势、近似重复的序列、形状、事件或频繁的子序列，所有这些都有相同的目标。Motif旨在发现子序列匹配数量最多的片段，通过提前定义的阀值$\varepsilon$，当所找的子序列片段之间的距离小于$\varepsilon$，就认为这些子序列片段为一个motif，如图1.1。 图1.1 时间序列的两个频繁模式，红色和绿色为一个Motif 栈式自编码器(SAE)定义：是一种无监督性的机器学习算法，其模型结构简单，只具有一个隐含层，网络层神经元之间是全连接的结构，如图1.2所示。研究者们对经典的自动编码器进行了很多优化的研究，提出了多种自动编码器的改良版本，如对原始自动编码器中隐含层的神经单元加以稀疏性的限制，这种网络被称为稀疏自动编码器。将经典的自动编码器进行叠加组合，则构成了栈式自动编码器。 图1.2 自动编码器网络示意图 栈式自动编码器是自动编码器的改进模型，由自动编码器的有效堆叠组合构成。网络模型对数据特征进行逐层的抽象提取，前一层自动编码器的输出(隐含层)结果将用于后一层自动编码器网络的输入，然后将自动编码器的编码部分进行叠加，然后叠加对应的编码部分。栈式自动编码器逐层对网络模型进行训练，每次对一个网络层以贪婪的思想进行训练，训练的算法为无监督的稀疏自动编码器。训练的过程参见图1.2，首先用自动编码器训练原始数据$X$$i$，得到网络模型的参数$W$$1$,$b$$1$和原始数据的一阶特征表示$h$$1$，然后将一阶特征$h$$1$作为下一个自动编码器网络的输入，训练得到第二个自动编码器的参数$W$$2$,$b$$2$和二阶特征表示$h$$2$,训练过程中，保持其他层的参数不变。最后，当训练完成后，通过微调整个网络模型，以改善整个网络模型的学习效果。 栈式自编码器提取时间序列特征的方法1.1 基本思路栈式自编码器广泛应用于无监督特征学习，本文将用于时间序列的特征提取。通过对上述栈式自动编码器的定义，我们可以将高维的时间序列转化为低维特征表示。我们先将时间序列切片然后转化为空间的一个向量，并用自动编码器提取低维特征。 本文利用栈式自编码器的基本思想为：对于一个大规模高维的时间序列，我们将时间序列按子序列切分为大规模的小片段时间序列向量，对大规模时间序列片段向量组成的矩阵进行深层的学习，使用神经网络学习时间点$t$$i$，$t$$m$（$i$和$m$是时间序列的任意时刻）之间的关联，比如$t$$5$，$t$$10$在时间序列中的一种特殊的关联，完成这两个点之间的特征学习，进一步特征提取，去理解多个点之间的关联性,最终学习出这个时间序列的关联特征。经过网络的训练之后，我们能得到一个模型，这个模型的输入是一个高维的时间序列片段，输出是一个低维的时间序列特征向量。模型如下，输入是$T[j:j+m]$,输出是$S[i:i+l]$,其中$m&gt;l$。 S[i:i+l]=f(T[j:j+m]) (1) 时间序列特征提取模型 自动编码器将时间序列的高维向量转化为低维向量，并且学习到的低维的特征向量包含了高维时间序列的本质特征，是原始时间序列的良好表示，也是原始时间序列的另一种特征表示，由此得到的特征向量可用于相似度计算以及motif的查找。算法流程分为3个步骤：首先，对时间序列进行预处理，对时间序列按一定的子序列长度进行切分，构建时间序列片段向量的空间模型，每个时间序列片段转化成空间中的一个向量。然后将这些高维的向量组成的矩阵输入到构建好的栈式自编码器中学习，经过逐层的抽象，提取不同抽象程度的低维特征向量。最后，利用传统的motif查找方法对抽取到的低维时间序列特征向量进行motif查找，验证原始方法找寻的motif与栈式特征提取后找寻motif的方法的准确度。 1.2 时间序列的预处理1.2.1 时间序列规范化数据的规范化包括归一化标准化正则化，是一个统称（也有人把标准化作为统称）。 数据规范化是数据挖掘中的数据变换的一种方式，数据变换将数据变换或统一成适合于数据挖掘的形式，将被挖掘对象的属性数据按比例缩放，使其落入一个小的特定区间内，如[-1, 1]或[0, 1]。对属性值进行规范化常用于涉及神经网络和距离度量的分类算法和聚类算法当中。比如使用神经网络后向传播算法进行分类挖掘时，对训练元组中度量每个属性的输入值进行规范化有利于加快学习阶段的速度。对于基于距离度量相异度的方法，数据归一化能够让所有的属性具有相同的权值。 数据规范化的常用方法有三种：最小最大值规范化，z-score标准化和按小数定标规范化。 在本文中由于使用的是sigmoid函数输出的是$[0，1]$，且在对神经网络进行训练的时候，应当把输入的数据处理成适合于网络训练的数据，这里需要对原始的时间序列进行放缩，进行归一化的原因是把各个特征的尺度控制在相同的范围内，这样可以便于找到最优解。文章用的是最小最大值规范化，由于使用的是sigmoid函数，因此对时间序列进行最小最大值规范化，公式如下： x^*=\frac{x-max}{max-min} (2) 公式（2）是最小最大值规范化的公式，该规范化将时间序列数据进行缩放，使数据范围在区间[0，1]上,对时间序列的伸缩变换的目的是使其不同度量之间的特征具有可比性，同时不改变原始数据的分布。 1.2.2 适合神经网络的时间序列子序列的片段向量矩阵公式（2）已经对时间序列数据进行了最小最大值规范化，但是这样的时间序列数据并不能适合于我们的自编码器神经网络的训练，因此我们还需要对数据进行处理，使构建的时间序列数据集适合于自编码器的训练。对时间序列的特征提取，或者说是motif查找时都是研究的时间序列的子序列，因此，我们在用栈式自编码器进行特征提取时，操作的应该是子序列，为了减少过拟合的问题，我们在截取时间序列片段时用了滑动窗口，大小是64，子序列片段设置为256，因此我们对一个完整的大的时间序列进行片段的切分，每256维构建一个向量空间模型，滑动窗口是64，就是每滑动64个维度进行一个向量模型的截取，最后这些截取的向量空间模型构成一个向量矩阵作为自编码器的训练数据集。 1.3 栈式自编码器栈式自编码器是一种多层的前传神经网络，可以用来对数据进行特征提取，同样也可以用于时间序列的特征提取，自编码器已经广泛运用于深度学习中，在进行时间序列特征提取时，本文使用了1个输入层、2个隐含层以及一个输出层组成的神经网络。以第一个隐含层为例，输入是一个256维的时间序列子片段向量$x$，输出层设置为128维的向量$z$，该一层$z$是自编码器降维后的特征向量，然后输出层是根据特征向量自编码器重构的256维向量$\hat{x}$，该隐藏层和输出层的激活情况公式为 z=sigmoid(Wx+b) (3) \hat{x}=sigmoid(W^Ty+b^‘) (4) 其中，$sigmoid(x)=1/(1-e^{-x})$为Sigmoid函数，其中$W$是自编码器的权重矩阵，$W^T$是解码器的权重矩阵，$b$和$b^‘$是偏置量。 作为无监督学习，自动编码器与监督学习不同，自动编码器的目标是输出层尽量重构输入层的状态，理想的情况下，隐藏层的特征能正确重构一个完全相同的输入层的时间序列片段，所以说隐藏层所得的特征向量即为原始时间序列的良好表示，但是重构不能保证100%的正确率，因此隐藏层作为提取的特征向量还是存在有一点误差，该误差来源于重构误差，训练时要使编码器尽可能减少重构误差。所以，自编码器重构的目标函数可以得出，公式为 \left\{ W,b,W^T,b^‘ \right\}=argmin\,Loss(x,\hat{x})=||x-\hat{x}||^2 (5) 其中，$Loss(x,\hat{x})$是损失函数，也叫重构误差，它的作用是衡量最后输出层与输入层重构的情况，我们自编码器要找寻的就是该误差的最小值。 隐藏层$z$的维度是小于输入层$x$的维度的，所以隐藏层$z$可以学习到输入样本的低维表示，并且通过解码能够包含与高维表示相同的信息。使用没有标注的数据集$X$，进行自动编码的无监督学习，最后对于任何输入向量$x$，通过训练好的模型进行计算，得到隐藏层向量$z$,即输入向量的一个低维编码。 自动编码器的权重训练采用随机梯度下降算法，使用公式为 W\leftarrow W-\eta\frac{\partial Loss(X,\hat{X})}{\partial W} (6) 该公式是梯度下降公式，用来更新权重矩阵，其中:$\eta$为更新的步长；其他参数$b,W^T,b^‘$采用相同的方式更新。本文更新的步长设置为0.001。 1.4 motif的查找方法通过大量文章的阅读，发现前人在motif的找寻中提供了大量的方法，为了方便进行结果的分析和比较，本文选用了一种思路就是matrix profile的方式，该方法是想计算出所有的距离然后形成的矩阵，例如，有一个时间序列T,T0是长度为m，起始位置为0的T的子序列，当用该T0与时间序列T中的所有子序列段作比较时，能得出一个距离向量d0;同理可得子序列段T1,T2,…,Tn-m+1的距离向量d1,d2,…,dn-m+1;这些距离向量共同构成了一个矩阵D。该矩阵保留了所有子序列之间的关系，我们要查询子序列之间的关系时，只需要通过矩阵的下标值就能得到相应子序列段的距离，例如:矩阵D[3][10]的元素值，即为T3与T10两个子序列片段的欧几里得距离，欧式公式为 d(x,y)= \sqrt{(x_1-y_1)^2+(x_2-y_2)^2+...+(x_n-y_n)^2}= \sqrt{\sum_{i=1}^n(x_i-y_i)^2} (7) 欧式公式是度量两个子序列之间的距离的，当距离越小时，代表两个片段越相似，同理，当距离越大时，两个片段越不相似。 栈式自编码器提取时间序列特征实现2.1 数据预处理的实现在这一步中我们着重构建一个适合于神经网络学习的数据集，构建一个适合于神经网络输入的向量矩阵，我们的目标是特征提取，当然我们不能直接输入整个时间序列，全部一次性进行特征提取，因此我们从子序列入手，在这一步，我们先读取准备好的时间序列文本。该数据集含有40m，初步估计有几百万个时间点数据。预处理的第一步是进行最小值最大值规范化，之后按长度m为256，滑动窗口window为64进行切分该时间序列T，切分后的时间序列应该是一个个长为256的子序列片段，T0,T64,…，然后按每k个样本进行合成矩阵，之后把这些矩阵合并成一个3维的空间模型F，这就完成了数据集的整理，流程如算法1 算法1 Procedure dataProcess(filePath) Input:时间序列T Output:适合于神经网络的3维空间模型F，包含大小为k的向量组成矩阵 T $\leftarrow$ getFile(filePath) t $\leftarrow$ MaxMinScore(T) M $\leftarrow$ TsSplitAndCombineMatrix(m,window,T) F $\leftarrow$ MCombineSpace(k) return F 对于算法1，我们最终得到的是一个3维的空间模型，getFile()函数可以读取时间序列文件返回原始时间序列，MaxMinScore()对时间序列进行最小值最大值规范化处理。TsSplitAndCombineMatrix()中的m是指子序列的片段大小，window是指滑动窗口大小，该方法返回的是以子序列向量组成的向量矩阵。MCombineSpace()中的k是每个合成矩阵的大小，返回的F是合成矩阵的集合。该空间模型也是神经网络的输入数据集。 2.2 栈式自编码器实现栈式自编码器的实现是本文的核心的步骤，首先我们的目标是输入一个256维的时间序列子片段，通过该栈式自编码器能够提取相应的特征向量。我们得设置一些神经网络的参数，在实现过程中，我们设置一个输入层，两个隐藏层，一个输出层，该栈式自编码网络提取的特征向量是第二个隐藏层，我们的输入层设置为256，第一个隐藏层的输出设置为128，然后激活函数用的是sigmoid()函数。 因此，第一层输入为X，第一层自编码器权重矩阵W1_e应该是[256,128]，相应的第一层的解码器权重矩阵W1_d应该是[128,256],第一层的偏置量b1_e应该是[128],第一层的解码器偏置量b1_d应该是[256]，第一层的输出为Z1，根据公式(3)(4)原理实现第一层，根据公式(5)来编写这一层的重构误差，利用BP梯度下降公式(6)进行这一层的网络学习训练，具体的算法2如下 算法2 Procedure firstHidden(F) Input:W1_e，W1_d，b1_e，b1_d Output:训练得到的第一层编码器的权重矩阵和偏置量（W1_e，W1_d，b1_e，b1_d） (W1_e，W1_d，b1_e，b1_d)$\leftarrow$ WeightInitialization(W1_e，W1_d，b1_e，b1_d) for each $X_i$ in F:{ Z1 $\leftarrow$ Encoder1(W1_e,b1_e,$X_i$) $\hat{X_i}$ $\leftarrow$ Decoder1(W1_d,b1_d,Z1) loss=caluteLoss($X_i$,$\hat{X_i}$) updateWeightBylearn() } return W1_e，W1_d，b1_e，b1_d 上述的算法2中WeightInitialization()函数是对W1_e，W1_d，b1_e，b1_d这些值初始化并根据均值为0,方差为0.01给这些权重初始值，Encoder()函数是编码器实现了公式(3)，返回的是输入的特征提取后的特征向量。Decoder()函数是解码器实现了公式(4)，返回的是特征向量重构的输入时间序列片段，caluteLoss()函数实现了公式(5)重构误差的计算，updateWeightBylearn()通过BP梯度下降公式(6)更新权重。算法2返回了一些训练好的权重参数可用于第二层的训练。 第二层隐藏层的输入来源于第一层隐藏层的输出，第二层的输入为Z1，由第一层的自编码器得到。同理，第二层的自编码器权重矩阵W2_e应该是[128,64]，相应的第二层的解码器是[64，128],第一层的偏置量b2_e应该是[64],第一层的解码器偏置量b2_d应该是[128]，第一层的输出为Z2，根据公式(3)(4)原理实现第二层，根据公式(5)来编写这一层的重构误差，利用BP梯度下降公式(6)进行这一层的网络学习训练，具体的算法3如下 算法3 Procedure secondHidden(W1_e,b1_e,F) Input:W1_e，b1_e,F,W2_e，W2_d，b2_e，b2_d Output:训练得到model (W2_e，W2_d，b2_e，b2_d)$\leftarrow$ WeightInitialization(W2_e，W2_d，b2_e，b2_d) for each $X_i$ in F:{ $Z1$ $\leftarrow$ Encoder1(W1_e,b1_e,$X_i$) $Z2$ $\leftarrow$ Encoder2(W2_e,b2_e,$Z1$) $\hat{Z1}$ $\leftarrow$ Decoder2(W1_d,b1_d,$Z2$) loss=caluteLoss($Z1$,$\hat{Z1}$) updateWeightBylearn() } model $\leftarrow$ saveModel() return model 上述算法3WeightInitialization()函数也是初始化第二层的权重，Z1是第一层隐藏层的编码，第二层是对第一层的重复利用，只不过输入的是第一层的编码，重构的是第一层的编码，同样该层也是运用BP梯度下降来更新权重，在训练完成后，我们需要通过saveModel()函数将模型保存下来。 通过算法1、2、3，我们已经完成了神经网络的训练，最终我们得到一个模型，该模型的功能如公式(1),其中m是256，l是64，所以当我们输入一个时间序列的子序列长度为256时，我们能通过模型直接得到64维的自编码器提取后的特征向量，至此神经网络的训练基本上完成，以下是验证该网络的效果和准确率。 2.3 原始方法找寻motif原始方法暴力求解距离矩阵可以找到正确的motif，这里我们的原始方法同样用的是欧式距离，当然我们也可以用相关系数的方法来找出motif，但是这里我们采用的是欧式距离，因为我们仅仅用来检测神经网络的降维效果。为了简单起见，我们用的测试数据点大约是60000个点，为了避免重复计算，我们建立滑动窗口，大小同样为64，所谓的暴力算法，就是使用循环来找寻这个距离矩阵，具体的原理可以参考上述1.4motif的查找方法，最后我们给原始方法设置一个阀值$\varepsilon$，例如，当我们找到矩阵中小于$\varepsilon$的值是D[3][10],矩阵的下标是记录的起始位置，当然这个位置得乘上滑动窗口，D[3][10]代表的是T[3 64:3 64+256]跟T[10 64:10 64+256]的两个子序列的距离，因为它们之间的距离小于$\varepsilon$，因此这两个子序列片段为一个motif。原始方法找寻motif的方法如算法4 算法4 Procedure findMotif(filePath) Input:时间序列T Output:motif T $\leftarrow$ getFile(filePath) M $\leftarrow$ TsSplitAndCombineMatrix(m,window,T) for i in range(0,len(M)){ for j in range(i,len(M)){ dist $\leftarrow$ calculateEd(M[i],M[j]) D[i][j] $\leftarrow$ dist } } motif $\leftarrow$ lessThanVarepsilon(D) return motif 上述算法是原始找寻motif的方法，T是读取文件得到的时间序列，M是以大小为256对时间序列进行切分，滑动窗口为64的向量矩阵，矩阵的第一行就是以0开始的大小为256的子序列，第二行是64开始的，长度为256的第二个子序列，依次下去。calculateEd()函数计算的是两个子序列的欧几里得距离，D是所有子序列之间的距离矩阵，lessThanVarepsilon()是找出距离矩阵D中小于阀值$\varepsilon$的值，即为motif。 2.4 特征提取后找寻的motif特征提取找寻motif只需在中间添加一个步骤就是神经网络提取特征的那一个步骤,把上面的算法加以改进就是神经网络降维找寻motif的方法了，如算法5 算法5 Procedure findMotif(filePath) Input:时间序列T Output:motif T $\leftarrow$ getFile(filePath) M $\leftarrow$ TsSplitAndCombineMatrix(m,window,T) for i in range(0,len(M)){ for j in range(i,len(M)){ S1 $\leftarrow$ model(M[i]) S2 $\leftarrow$ model(M[j]) dist $\leftarrow$ calculateEd(S1,S2) D[i][j] $\leftarrow$ dist } } motif $\leftarrow$ lessThanVarepsilon(D) return motif 上述算法添加了特征提取的model()那个步骤，目的是验证看神经网络进行特征提取之后特征向量跟原始时间序列是否保持原有的特征，也想通过代码验证这种自编码的方法在实现特征提取是否为原始序列的另一种良好表述。 实验结果及分析实验环境我们的实验采用的是macos系统，神经网络的编写用的是tensorflow，TensorFlow™是一个基于数据流编程（dataflow programming）的符号数学系统，被广泛应用于各类机器学习（machine learning），算法的编程实现编译工具是pycharm，是一个主流的编译工具，编写的语言是python，该语言简练，简洁。 数据集实验时，我们采用的数据集是一个带有40m的文本文件，保守估计该数据集的数据点有几百万个。当然神经网络训练的数据集是通过该文本处理得到的，预处理的方法可参考算法1。其中我们提取该时间序列的前69000个数据点作为测试数据集。 验证神经网络所找的motif的准确性由于通过神经网络特征提取后阀值的选取不能跟原始序列保持一致性，故我们在进行神经网络进行时应该先得出距离矩阵，通过算法1、2、3，我们训练好神经网络模型，通过各种参数的尝试，得到了一个效果最好的模型，该模型训练时步长为0.001，权重的初始值是服从正态分布的，均值为0，方差为0.01，k的值为20。输入层为256，第一个隐藏层输出是128，第二个是64。滑动窗口设置为64。由于通过神经网络特征向量之间的欧式距离变得相当小，我们通过观察距离矩阵找到阀值$\varepsilon$，然后以这个阀值$\varepsilon$为标准找相应的motif，其中的motif结果如下 图1.3 自编码器特征提取后找到的motif1 图1.4 自编码器特征提取后找到的motif2 上述第一个motif是通过神经网络进行降维后找到的，我们的算法找到的motif的两个片段的起始位置分别是（64 6144）（64 15168）（64 15232）（64 15552），第二个motif的（192 256）（192 1216）（192 1280），尽管所截取的motif带有相邻的片段，但是也不影响我们神经网络特征的结果是准确的，至少这种方法进行特征提取后并没有丢失原始时间序列的本质特征，即神经网络降维后的特征向量是原来序列的良好表示。 验证神经网络的准确率上面我们提到通过神经网络特征提取后阀值的选取不能跟原始序列保持一致性，那么我们怎么才能进行准确率的估计呢？我们可以从原始方法得到的距离矩阵和神经网络得到的距离矩阵入手，在进行算法4我们可以得到原始方法得到的距离矩阵，同理算法5我们可以得到神经网络哦的距离矩阵。但是这两个矩阵中的值大不相同，当然阀值是不能同步的，那什么样的方法能保证大家取的相似度一致呢，我们可以对两个距离矩阵进行归一化，把范围规定为[0,1]之间，然后我们找两个矩阵归一化后阀值小于0.2的值，这样，我们就能保持一致性，那么设N是原始方法找到的motif个数，n是和原始方法找到的motif相同的值。则我们的准确率 $p=\frac{n}{N}*100\%$（8） 通过实验结果，我们得到的n是7100，N是7536，故得到对69000个点进行实验，实验的准确率为94%。 大量实验找寻原始阀值和特征向量阀值关系本文一个难点就是要找寻原始时间序列子序列片段之间的距离与特征向量之间的阀值的关系，我们都知道神经网络是一个黑盒子，要找寻特征向量与原始时间序列的关系是很难实现的，由于神经网络权重的设置是没有意义的，权重矩阵的每一个权重都是没有意义的，不像物理中的一些参数带有实际意义，因此要找寻改变后的关系是很难的，但是我们能通过实验大概得出原始的时间序列片段间距离与特征向量间距离的关系，要找寻这样的关系需要进行大量的实验。实验中，我们每次增大测试数据的点数，每次增加1000，增加20次，每一次得到原始方法的距离矩阵和特征向量的距离矩阵。将原始方法距离矩阵的里的值除以特征向量的距离矩阵相应位置的值，最后求这些值的平均值，重复20次，得到20个值，再求这20个值的均值，最后得到一个参数e=1696。我们设Xi，Xj为原始序列的两个子片段，他们的欧式距离定义为d(Xi，Xj),通过神经网络特征提取后他们的特征向量分别为xi，xj，他们的欧式距离定义为d(xi，xj)。则这两个欧式距离的关系可近似认为 d(X_i，X_j)=d(x_i,x_j)*e（9） 之后通过实验，我们设置原始方法的阀值为4，然后找到的motif个数为5633，然后再特征向量的距离矩阵中我们把里面的值同时乘以e，同样找寻阀值小于4的4795。然后通过公式（8）计算得到准确率为85%。这个得到的准确率的误差有两个，一个是在神经网络训练时我们在重构原始数据时会有一定的重构误差，另一种情况就是我们所找的关系e中所产生的误差，真正的e不是一个确定的值，它是一个变动的值，单单取均值不能保证真正的关系。 结语利用自编码器进行时间序列的特征踢是一种新的降维的方法，它给研究者在研究motif时提供了一种新的思路，人工智能，深度学习作用越来越广泛。本文所提出的方法还需要作很多的改进，该自编码的方法还能应用于很多领域，还有待探索研究。]]></content>
      <categories>
        <category>1</category>
      </categories>
      <tags>
        <tag>1</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2019%2F07%2F10%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
