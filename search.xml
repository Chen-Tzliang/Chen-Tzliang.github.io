<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[对自编码及时间序列特征提取的总结]]></title>
    <url>%2F2019%2F07%2F15%2F%E5%AF%B9%E8%87%AA%E7%BC%96%E7%A0%81%E5%8F%8A%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%E7%9A%84%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[介绍时间序列是一种应用广、复杂、维度高的数据形式，是根据客观对象的某些物理量在时间维度上的采样值排列组成的数据序列，按一定的时间走势的数据类型，它客观地记录了所观测对象在各个单位时间点上的状态值。时间序列应用十分的广泛，金融数据，流媒体数据、气象数据、人口普查数据、系统日志数据都是时间序列数据类型。而找寻相似的时间序列频繁模式在时间序列的挖掘中有相当重要的意义，例如，我们要找寻某一个股票的走势，或者对股票进行分析，我们需要找出这只股票的一些频繁的周期，也叫motif查询，就是找出股票中走势相似的某些时间序列片段，那么，我们要找寻这些片段就需要用到距离测量的算法，例如欧式距离，相关系数距离等，然后通过距离阀值找到小于某个阀值的片段。在找寻这些片段时可能要花费大量的时间，这时我们就需要到一些降维的技术来缩短，本文所讨论的是单变量的时间序列，在传统的方法中，我们有paa分段技术，符号化sax，主成分分析法pca等等，这些都是传统的降维方法，本文通过大量文章的阅读，提出了用自编码器研究时间序列特征提取，并大量实验测量降维后的特征向量是否保持原有的时间序列的特征。 定义问题定义在motif或者进行时间序列片段的相似度计算的时候，降低时间序列的维度对于分析时间序列有着极其重要的作用，时间序列的特征提取能很大限度的减少时间序列距离测量的时间，因此，我们指在研究自编码器对时间序列的特征的提取。栈式自编码器特征提取是指利用栈式自编码器对时间序列进行提取，对于高维的时间序列的片段，我们通过训练栈式自编码器，使该神经网络能够从输入的时间序列片段中得到一个低维的特征向量的输出，再次，我们利用该低维的特征向量完成motif的查找，并减少通过高维暴力计算找寻motif的时间，并进行大量实验验证特征提取后的阀值与原始阀值的近似关系。 基本定义时间序列定义：时间序列T是按时间顺序排列的数据点序列。$T=$[($t$$1$,$d$$1$),($t$$2$,$d$$2$),…,($t$$n$,$d$$n$)],其中d是时间$t$的数据项，$n$是时间序列的长度。 时间子序列定义：子序列$T[j:j+m]$ = [$d$$j$,$d$$j+1$,…,$d$$j+m$]是从位置$j$开始，长度为$m$的$T$中的一组连续点，本文只考虑时间序列的数据值。 时间序列motif定义：Motif被描述为重复的模式、频繁的趋势、近似重复的序列、形状、事件或频繁的子序列，所有这些都有相同的目标。Motif旨在发现子序列匹配数量最多的片段，通过提前定义的阀值$\varepsilon$，当所找的子序列片段之间的距离小于$\varepsilon$，就认为这些子序列片段为一个motif，如图1.1。 图1.1 时间序列的两个频繁模式，红色和绿色为一个Motif 栈式自编码器(SAE)定义：是一种无监督性的机器学习算法，其模型结构简单，只具有一个隐含层，网络层神经元之间是全连接的结构，如图1.2所示。研究者们对经典的自动编码器进行了很多优化的研究，提出了多种自动编码器的改良版本，如对原始自动编码器中隐含层的神经单元加以稀疏性的限制，这种网络被称为稀疏自动编码器。将经典的自动编码器进行叠加组合，则构成了栈式自动编码器。 图1.1 自动编码器网络示意图 栈式自动编码器是自动编码器的改进模型，由自动编码器的有效堆叠组合构成。网络模型对数据特征进行逐层的抽象提取，前一层自动编码器的输出(隐含层)结果将用于后一层自动编码器网络的输入，然后将自动编码器的编码部分进行叠加，然后叠加对应的编码部分。栈式自动编码器逐层对网络模型进行训练，每次对一个网络层以贪婪的思想进行训练，训练的算法为无监督的稀疏自动编码器。训练的过程参见图1.2，首先用自动编码器训练原始数据$X$$i$，得到网络模型的参数$W$$1$,$b$$1$和原始数据的一阶特征表示$h$$1$，然后将一阶特征$h$$1$作为下一个自动编码器网络的输入，训练得到第二个自动编码器的参数$W$$2$,$b$$2$和二阶特征表示$h$$2$,训练过程中，保持其他层的参数不变。最后，当训练完成后，通过微调整个网络模型，以改善整个网络模型的学习效果。 栈式自编码器提取时间序列特征的方法1.1 基本思路栈式自编码器广泛应用于无监督特征学习，本文将用于时间序列的特征提取。通过对上述栈式自动编码器的定义，我们可以将高维的时间序列转化为低维特征表示。我们先将时间序列切片然后转化为空间的一个向量，并用自动编码器提取低维特征。 本文利用栈式自编码器的基本思想为：对于一个大规模高维的时间序列，我们将时间序列按子序列切分为大规模的小片段时间序列向量，对大规模时间序列片段向量组成的矩阵进行深层的学习，使用神经网络学习时间点$t$$i$，$t$$m$（$i$和$m$是时间序列的任意时刻）之间的关联，比如$t$$5$，$t$$10$在时间序列中的一种特殊的关联，完成这两个点之间的特征学习，进一步特征提取，去理解多个点之间的关联性,最终学习出这个时间序列的关联特征。经过网络的训练之后，我们能得到一个模型，这个模型的输入是一个高维的时间序列片段，输出是一个低维的时间序列特征向量。模型如下，输入是$T[j:j+m]$,输出是$S[i:i+l]$,其中$m&gt;l$。 $S[i:i+l]=f(T[j:j+m])$ (1) 时间序列特征提取模型 自动编码器将时间序列的高维向量转化为低维向量，并且学习到的低维的特征向量包含了高维时间序列的本质特征，是原始时间序列的良好表示，也是原始时间序列的另一种特征表示，由此得到的特征向量可用于相似度计算以及motif的查找。算法流程分为3个步骤：首先，对时间序列进行预处理，对时间序列按一定的子序列长度进行切分，构建时间序列片段向量的空间模型，每个时间序列片段转化成空间中的一个向量。然后将这些高维的向量组成的矩阵输入到构建好的栈式自编码器中学习，经过逐层的抽象，提取不同抽象程度的低维特征向量。最后，利用传统的motif查找方法对抽取到的低维时间序列特征向量进行motif查找，验证原始方法找寻的motif与栈式特征提取后找寻motif的方法的准确度。 1.2 时间序列的预处理1.2.1 时间序列规范化数据的规范化包括归一化标准化正则化，是一个统称（也有人把标准化作为统称）。 数据规范化是数据挖掘中的数据变换的一种方式，数据变换将数据变换或统一成适合于数据挖掘的形式，将被挖掘对象的属性数据按比例缩放，使其落入一个小的特定区间内，如[-1, 1]或[0, 1]。对属性值进行规范化常用于涉及神经网络和距离度量的分类算法和聚类算法当中。比如使用神经网络后向传播算法进行分类挖掘时，对训练元组中度量每个属性的输入值进行规范化有利于加快学习阶段的速度。对于基于距离度量相异度的方法，数据归一化能够让所有的属性具有相同的权值。 数据规范化的常用方法有三种：最小最大值规范化，z-score标准化和按小数定标规范化。 在本文中由于使用的是sigmoid函数输出的是$[0，1]$，且在对神经网络进行训练的时候，应当把输入的数据处理成适合于网络训练的数据，这里需要对原始的时间序列进行放缩，进行归一化的原因是把各个特征的尺度控制在相同的范围内，这样可以便于找到最优解。文章用的是最小最大值规范化，由于使用的是sigmoid函数，因此对时间序列进行最小最大值规范化，公式如下： x^*=\frac{x-max}{max-min}$$ (2) 公式（2）是最小最大值规范化的公式，该规范化将时间序列数据进行缩放，使数据范围在区间[0，1]上,对时间序列的伸缩变换的目的是使其不同度量之间的特征具有可比性，同时不改变原始数据的分布。 #### 1.2.2 适合神经网络的时间序列子序列的片段向量矩阵 公式（2）已经对时间序列数据进行了最小最大值规范化，但是这样的时间序列数据并不能适合于我们的自编码器神经网络的训练，因此我们还需要对数据进行处理，使构建的时间序列数据集适合于自编码器的训练。对时间序列的特征提取，或者说是motif查找时都是研究的时间序列的子序列，因此，我们在用栈式自编码器进行特征提取时，操作的应该是子序列，为了减少过拟合的问题，我们在截取时间序列片段时用了滑动窗口，大小是64，子序列片段设置为256，因此我们对一个完整的大的时间序列进行片段的切分，每256维构建一个向量空间模型，滑动窗口是64，就是每滑动64个维度进行一个向量模型的截取，最后这些截取的向量空间模型构成一个向量矩阵作为自编码器的训练数据集。 ### 1.3 栈式自编码器 栈式自编码器是一种多层的前传神经网络，可以用来对数据进行特征提取，同样也可以用于时间序列的特征提取，自编码器已经广泛运用于深度学习中，在进行时间序列特征提取时，本文使用了1个输入层、2个隐含层以及一个输出层组成的神经网络。以第一个隐含层为例，输入是一个256维的时间序列子片段向量$x$，输出层设置为128维的向量$z$，该一层$z$是自编码器降维后的特征向量，然后输出层是根据特征向量自编码器重构的256维向量$\hat{x}$，该隐藏层和输出层的激活情况公式为 $$z=sigmoid(Wx+b)$$ (3) $$\hat{x}=sigmoid(W^Ty+b^‘)$$ (4) 其中，$sigmoid(x)=1/(1-e^{-x})$为Sigmoid函数，其中$W$是自编码器的权重矩阵，$W^T$是解码器的权重矩阵，$b$和$b^‘$是偏置量。 作为无监督学习，自动编码器与监督学习不同，自动编码器的目标是输出层尽量重构输入层的状态，理想的情况下，隐藏层的特征能正确重构一个完全相同的输入层的时间序列片段，所以说隐藏层所得的特征向量即为原始时间序列的良好表示，但是重构不能保证100%的正确率，因此隐藏层作为提取的特征向量还是存在有一点误差，该误差来源于重构误差，训练时要使编码器尽可能减少重构误差。所以，自编码器重构的目标函数可以得出，公式为 $\left\{ W,b,W^T,b^‘ \right\}=argmin\,Loss(x,\hat{x})=||x-\hat{x}||^2$ (5) 其中，$Loss(x,\hat{x})$是损失函数，也叫重构误差，它的作用是衡量最后输出层与输入层重构的情况，我们自编码器要找寻的就是该误差的最小值。 隐藏层$z$的维度是小于输入层$x$的维度的，所以隐藏层$z$可以学习到输入样本的低维表示，并且通过解码能够包含与高维表示相同的信息。使用没有标注的数据集$X$，进行自动编码的无监督学习，最后对于任何输入向量$x$，通过训练好的模型进行计算，得到隐藏层向量$z$,即输入向量的一个低维编码。 自动编码器的权重训练采用随机梯度下降算法，使用公式为 $$W\leftarrow W-\eta\frac{\partial Loss(X,\hat{X})}{\partial W}$$ (6) 该公式是梯度下降公式，用来更新权重矩阵，其中:$\eta$为更新的步长；其他参数$b,W^T,b^‘$采用相同的方式更新。本文更新的步长设置为0.001。 ### 1.4 motif的查找方法 通过大量文章的阅读，发现前人在motif的找寻中提供了大量的方法，为了方便进行结果的分析和比较，本文选用了一种思路就是matrix profile的方式，该方法是想计算出所有的距离然后形成的矩阵，例如，有一个时间序列$T$，$T_0$是长度为m，起始位置为0的T的子序列，当用该$T_0$与时间序列$T$中的所有子序列段作比较时，能得出一个距离向量$D_0$。同理可得子序列段$T_1$,$T_2$,...,$T_{n-m+1}$的距离向量$d_1$,$d_2$,...,$d_{n-m+1}$。这些距离向量共同构成了一个矩阵D。该矩阵保留了所有子序列之间的关系，我们要查询子序列之间的关系时，只需要通过矩阵的下标值就能得到相应子序列段的距离，例如:矩阵D[3][10]的元素值，即为$T_3$与$T_{10}$两个子序列片段的欧几里得距离，欧式公式为 $$d(x,y)= \sqrt{(x_1-y_1)^2+(x_2-y_2)^2+...+(x_n-y_n)^2}= \sqrt{\sum_{i=1}^n(x_i-y_i)^2}欧式公式是度量两个子序列之间的距离的，当距离越小时，代表两个片段越相似，同理，当距离越大时，两个片段越不相似。 栈式自编码器提取时间序列特征实现]]></content>
      <categories>
        <category>论文（学术）</category>
      </categories>
      <tags>
        <tag>论文（学术）</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2019%2F07%2F10%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
